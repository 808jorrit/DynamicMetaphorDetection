{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XbnY8OPWP4oU",
    "outputId": "ad825eab-aea6-421d-fdf5-799fd9aaa926"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USCQADIOteli"
   },
   "outputs": [],
   "source": [
    "test = \"On the question of foreign trade, previous leaders were guided by a shameful policy of capitulation, submission, and retreat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHOvuFLGQI19",
    "outputId": "39b78b70-df05-4ddb-d97e-d42c4da1bca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 400001 word vectors!\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe word vectors\n",
    "def load_glove_embeddings(glove_file_path):\n",
    "    embeddings_dict = {}\n",
    "    try:\n",
    "        with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.strip().split()\n",
    "                word = values[0]\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings_dict[word] = vector\n",
    "        print(f\"Got {len(embeddings_dict)} word vectors!\")\n",
    "        return embeddings_dict\n",
    "    except Error:\n",
    "        return None\n",
    "\n",
    "# Path to GloVe file\n",
    "glove_file = \"/content/glove.6B.50d.txt\"\n",
    "glove_embeddings = load_glove_embeddings(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWoSeCM7WUy0"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def find_outlier_words(text):\n",
    "    # tokenize\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    words = []\n",
    "    for word in doc:\n",
    "        if word.is_alpha and not word.is_stop:\n",
    "            words.append(word.text.lower())\n",
    "\n",
    "    if not words:\n",
    "        return [], [], [], []\n",
    "\n",
    "    # Glove-based\n",
    "    word_vectors = []\n",
    "    good_words_glove = []\n",
    "    for word in words:\n",
    "        if word in glove_embeddings:\n",
    "            word_vectors.append(glove_embeddings[word])\n",
    "            good_words_glove.append(word)\n",
    "        else:\n",
    "            print(f\"Skipping '{word}' - not in GloVe word list\")\n",
    "\n",
    "    glove_outliers = []\n",
    "    if word_vectors:\n",
    "        similarities = cosine_similarity(word_vectors)\n",
    "        avg_similarities = []\n",
    "        for i in range(len(good_words_glove)):\n",
    "            other_sims = [similarities[i][j] for j in range(len(good_words_glove)) if i != j]\n",
    "            avg_sim = np.mean(other_sims) if other_sims else 0\n",
    "            avg_similarities.append(avg_sim)\n",
    "\n",
    "        mean_similarity = np.mean(avg_similarities)\n",
    "        if mean_similarity > 0.7:  # Trial-and-error\n",
    "            print(\"High semantic coherence - likely no metaphors\")\n",
    "            return \"glove\", [], \"wordnet\", [], \"sbert\", []\n",
    "\n",
    "        # outlier detection\n",
    "        avg_all = np.mean(avg_similarities)\n",
    "        std_all = np.std(avg_similarities)\n",
    "        cutoff = avg_all - std_all\n",
    "        for i in range(len(good_words_glove)):\n",
    "            if avg_similarities[i] < cutoff:\n",
    "                glove_outliers.append((good_words_glove[i], avg_similarities[i]))\n",
    "\n",
    "    # wordnet\n",
    "    good_words_wordnet = []\n",
    "    for word in words:\n",
    "        if wn.synsets(word):\n",
    "            good_words_wordnet.append(word)\n",
    "        else:\n",
    "            print(f\"'{word}'not in wordnet\")\n",
    "\n",
    "    wordnet_outliers = []\n",
    "    if len(good_words_wordnet) > 1:\n",
    "        # Calculate pairwise WordNet similarities\n",
    "        similarities = []\n",
    "        for i, word1 in enumerate(good_words_wordnet):\n",
    "            sim_row = []\n",
    "            syn1 = wn.synsets(word1)[0]  # Take first synset for simplicity\n",
    "            for j, word2 in enumerate(good_words_wordnet):\n",
    "                if i == j:\n",
    "                    sim_row.append(1.0)  # Same word, max similarity\n",
    "                    continue\n",
    "                syn2 = wn.synsets(word2)[0]\n",
    "                # Compute Wu-Palmer similarity (returns None if no path)\n",
    "                sim = syn1.wup_similarity(syn2)\n",
    "                sim_row.append(sim if sim is not None else 0.0)\n",
    "            similarities.append(sim_row)\n",
    "\n",
    "        # Average similarities for each word\n",
    "        avg_similarities = []\n",
    "        for i in range(len(good_words_wordnet)):\n",
    "            other_sims = [similarities[i][j] for j in range(len(good_words_wordnet)) if i != j]\n",
    "            avg_sim = np.mean(other_sims) if other_sims else 0\n",
    "            avg_similarities.append(avg_sim)\n",
    "\n",
    "        # Find cutoff for \"outlier\" words\n",
    "        avg_all = np.mean(avg_similarities)\n",
    "        std_all = np.std(avg_similarities)\n",
    "        cutoff = avg_all - std_all\n",
    "\n",
    "        # Pick out outlier words\n",
    "        for i in range(len(good_words_wordnet)):\n",
    "            if avg_similarities[i] < cutoff:\n",
    "                wordnet_outliers.append((good_words_wordnet[i], avg_similarities[i]))\n",
    "\n",
    "    # --- SBERT-based outlier detection ---\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    full_sentence = \" \".join(words)\n",
    "    sbert_outliers = []\n",
    "    if full_sentence.strip():\n",
    "        full_embedding = model.encode(full_sentence)\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            leave_one_out = [w for j, w in enumerate(words) if j != i]\n",
    "            loo_sentence = \" \".join(leave_one_out)\n",
    "\n",
    "            if loo_sentence.strip():\n",
    "                loo_embedding = model.encode(loo_sentence)\n",
    "                similarity = cosine_similarity([full_embedding], [loo_embedding])[0][0]\n",
    "                if similarity < 0.8:  # Tune this threshold\n",
    "                    sbert_outliers.append((word, similarity))\n",
    "    else:\n",
    "        print(\"No valid SBERT sentence!\")\n",
    "\n",
    "    return \"glove\", glove_outliers, \"wordnet\", wordnet_outliers, \"sbert\", sbert_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_n1WYYoRckM"
   },
   "outputs": [],
   "source": [
    "sentence = (\"Hope is the last candle in the dark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5HUngiURJsw",
    "outputId": "2c0457ed-e7f5-4ea9-eae3-acbb8e5bd67b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('glove',\n",
       " [('candle', np.float32(0.28746852))],\n",
       " 'wordnet',\n",
       " [('candle', np.float64(0.11805555555555555))],\n",
       " 'sbert',\n",
       " [('candle', np.float32(0.6933816))])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_outlier_words(sentence)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
