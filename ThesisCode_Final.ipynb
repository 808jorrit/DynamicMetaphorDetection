{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c366c-27a2-49a4-be68-0bbf00a39875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet as wn\n",
    "from functools import lru_cache\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18dccb1-214d-43b9-977e-27b93245667f",
   "metadata": {},
   "source": [
    "loading + dropping irrelevant + merging the sentences back to their original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e5688-d30c-4313-8c0a-12281fc17346",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = pd.read_excel(\"/Users/j/Documents/thesis/dyn.xlsx\", sheet_name=\"trump\")\n",
    "clinton = pd.read_excel(\"/Users/j/Documents/thesis/dyn.xlsx\", sheet_name=\"clinton\")\n",
    "df = pd.concat([trump, clinton], ignore_index=True)\n",
    "\n",
    "df = df.loc[:, ~df.columns.str.startswith('SD_')]\n",
    "df = df.iloc[:, 19:].reset_index(drop=True)\n",
    "df['sentence'] = df['collocate left'].astype(str).fillna('') + ' ' + df['Kwic'].astype(str).fillna('') + ' ' + df['collocate right'].astype(str).fillna('')\n",
    "\n",
    "df2 = df[['sentence', ' basic form  of  potential metaphorical word with word class', 'POS tag following CLAWS7 of COCA', 'metaphor type']]\n",
    "df2['sentence'] = df2['sentence'].str.lower().fillna('')\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b6716-6331-4fa8-9c5b-99c4219848ec",
   "metadata": {},
   "source": [
    "Umbrella terms (this code was not used in the final product, it needs refinement and improvement. It did reach somewhat of an estimation of the target domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f441f83-ff16-4645-8a52-7721c55929ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_umbrella_term(sentence, glove_embeddings, noun_boost=2.0):\n",
    "    # Tokenize sentence\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Remove stopwords and get POS tags\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tagged_words = pos_tag([word for word in words if word.isalnum() and word not in stop_words])\n",
    "    \n",
    "    # Define relevant POS tags\n",
    "    relevant_pos = {'NN', 'NNS', 'NNP', 'NNPS','VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ','JJ', 'JJR', 'JJS','RB', 'RBR', 'RBS'}\n",
    "    relevant_words = [(word, pos) for word, pos in tagged_words if pos in relevant_pos]\n",
    "    \n",
    "    if not relevant_words:\n",
    "        return None\n",
    "    \n",
    "    # Filter words that exist in GloVe embeddings\n",
    "    valid_entries = [(word, pos) for word, pos in relevant_words if word in glove_embeddings]\n",
    "    if not valid_entries:\n",
    "        return None\n",
    "    \n",
    "    valid_words, valid_pos = zip(*valid_entries)\n",
    "    # Get embeddings for valid words\n",
    "    word_vectors = np.array([glove_embeddings[word] for word in valid_words])\n",
    "    \n",
    "    # Compute base attention weights based on vector norms\n",
    "    norm_weights = np.linalg.norm(word_vectors, axis=1)\n",
    "    \n",
    "    # Apply POS-based weighting (boost nouns)\n",
    "    pos_weights = np.array([noun_boost if pos in {'NN', 'NNS', 'NNP', 'NNPS'} else 1.0 \n",
    "                           for pos in valid_pos])\n",
    "    \n",
    "    # Combine weights (norm * POS boost)\n",
    "    weights = norm_weights * pos_weights\n",
    "    weights = weights / weights.sum()  # Normalize to sum to 1\n",
    "    \n",
    "    # Calculate weighted mean vector\n",
    "    weighted_mean = np.average(word_vectors, axis=0, weights=weights)\n",
    "    \n",
    "    # Calculate cosine similarity between weighted mean and each word vector\n",
    "    similarities = {}\n",
    "    for word, word_vector in zip(valid_words, word_vectors):\n",
    "        similarity = np.dot(weighted_mean, word_vector) / (\n",
    "            np.linalg.norm(weighted_mean) * np.linalg.norm(word_vector)\n",
    "        )\n",
    "        similarities[word] = similarity\n",
    "    \n",
    "    # Return word with highest similarity to the weighted mean\n",
    "    if similarities:\n",
    "        umbrella_term = max(similarities, key=similarities.get)\n",
    "        return umbrella_term\n",
    "    return None\n",
    "\n",
    "# Apply the function with progress tracking\n",
    "tqdm.pandas()  # Enable pandas integration with tqdm\n",
    "df2['umbrella_term'] = df2['sentence'].progress_apply(\n",
    "    lambda x: get_umbrella_term(x, glove_embeddings, noun_boost=2.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c970d-b0ab-4cd8-96e1-fc2aff1b6b86",
   "metadata": {},
   "source": [
    "Preprocessing code; 1. fix for abreviations/contractions, 'doesnt' becomes 'does not' etc. 2. fix for 'Q' at start of sentence (for interview-like strings in the dataset). 3. Remove random numbers 4. remove other symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d4a05-02cb-4567-8739-b746d2e44542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of common contractions\n",
    "contractions_dict = {\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"ain't\": \"is not\",\n",
    "}\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    # remove Q symbol which appears at start of some sentences\n",
    "    if sentence.startswith(\"q\"):\n",
    "        sentence = sentence[1:].strip()\n",
    "\n",
    "    # Remove single number symbols\n",
    "    sentence = re.sub(r'\\b\\d{1}\\b', '', sentence)\n",
    "\n",
    "    # Replace contractions with dict above. \n",
    "    words = sentence.split()\n",
    "    sentence = ' '.join([contractions_dict.get(word, word) for word in words])\n",
    "\n",
    "    # Remove other symbols (such as dot, question mark etc)\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply the processing function to each sentence in the 'sentences' column\n",
    "df2['processed_sentences'] = df2['sentence'].progress_apply(lambda x: process_sentence(str(x)) if x != 'nan' else x)\n",
    "\n",
    "# Tokenize the sentences\n",
    "df2['tokenized_sentences'] = df2['processed_sentences'].progress_apply(nltk.word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96248d-5de0-45c0-a74e-91b4c2f345e9",
   "metadata": {},
   "source": [
    "Loading glove embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f2900-b41f-4228-a18f-3a9e044dd9d0",
   "metadata": {},
   "source": [
    "download the glove embeddings from: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50e4eb-f153-4b1a-afed-624b1416ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe word vectors\n",
    "def load_glove_embeddings(glove_file_path):\n",
    "    embeddings_dict = {}\n",
    "    with open(glove_file_path, 'r') as f:\n",
    "        for line_num, line in enumerate(f, start=1):\n",
    "            values = line.strip().split()\n",
    "            if len(values) < 2:\n",
    "                continue  \n",
    "            try:\n",
    "                word = values[0]\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings_dict[word] = vector\n",
    "            except ValueError:\n",
    "                print(f\"Error: skipping line {line_num}.\")\n",
    "    print(f\"succes (denk ik)\")\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_file = \"/Users/j/Documents/thesis/glove.840B.300d.txt\"\n",
    "glove_embeddings = load_glove_embeddings(glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8db63-cf2d-4db5-89ec-4dadc6531ed5",
   "metadata": {},
   "source": [
    "Compare GLOVE embeddings of words to embedding of  TOPIC word(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fb2f3-d55c-4275-a866-5e52b2aa07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code will be added later ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d452d97-c3ea-489a-b77b-e0a83ca779b1",
   "metadata": {},
   "source": [
    "compare GLOVE - pairwise, all words vs all words. append mean similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b8f02-1d14-4d17-ab7c-5f8d16211132",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code will be added later ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f2dae-45f9-4e10-8b58-e027e532e6e5",
   "metadata": {},
   "source": [
    "compare WORDNET - TOPIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec6125-5822-4406-9009-64cd07ef9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "# Similarity computation between 2 synsets with fallbacks\n",
    "def compute_similarities(word_synset, topic_synset):\n",
    "    similarities = {'WUP': None, 'PATH': None, 'LCH': None, 'JC': None, 'LIN': None}\n",
    "    \n",
    "    try:\n",
    "        similarities['WUP'] = word_synset.wup_similarity(topic_synset)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        similarities['PATH'] = word_synset.path_similarity(topic_synset)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        similarities['LCH'] = word_synset.lch_similarity(topic_synset)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        similarities['JC'] = word_synset.jcn_similarity(topic_synset, brown_ic)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        similarities['LIN'] = word_synset.lin_similarity(topic_synset, brown_ic)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "def compute_df_similarities(df):\n",
    "    topic_word = df['umbrella_term'].iloc[0]\n",
    "    topic_synset = get_first_noun_synset(topic_word)\n",
    "    \n",
    "    wup_list = []\n",
    "    path_list = []\n",
    "    lch_list = []\n",
    "    jc_list = []\n",
    "    lin_list = []\n",
    "    \n",
    "    for tokens in tqdm(df['tokenized_sentences'], desc=\"Computing WordNet Similarities\"):\n",
    "        wup_vals, path_vals, lch_vals, jc_vals, lin_vals = [], [], [], [], []\n",
    "\n",
    "        for word in tokens:\n",
    "            word_synset = get_first_noun_synset(word)\n",
    "            if word_synset and topic_synset:\n",
    "                sims = compute_similarities(word_synset, topic_synset)\n",
    "                wup_vals.append(sims['WUP'] if sims['WUP'] is not None else 0)\n",
    "                path_vals.append(sims['PATH'] if sims['PATH'] is not None else 0)\n",
    "                lch_vals.append(sims['LCH'] if sims['LCH'] is not None else 0)\n",
    "                jc_vals.append(sims['JC'] if sims['JC'] is not None else 0)\n",
    "                lin_vals.append(sims['LIN'] if sims['LIN'] is not None else 0)\n",
    "            else:\n",
    "                wup_vals.append(0)\n",
    "                path_vals.append(0)\n",
    "                lch_vals.append(0)\n",
    "                jc_vals.append(0)\n",
    "                lin_vals.append(0)\n",
    "\n",
    "        wup_list.append(wup_vals if any(wup_vals) else [sum(wup_vals)/len(wup_vals)])\n",
    "        path_list.append(path_vals if any(path_vals) else [sum(path_vals)/len(path_vals)])\n",
    "        lch_list.append(lch_vals if any(lch_vals) else [sum(lch_vals)/len(lch_vals)])\n",
    "        jc_list.append(jc_vals if any(jc_vals) else [sum(jc_vals)/len(jc_vals)])\n",
    "        lin_list.append(lin_vals if any(lin_vals) else [sum(lin_vals)/len(lin_vals)])\n",
    "\n",
    "    df['wn_topic__WUP'] = wup_list\n",
    "    df['wn_topic__PATH'] = path_list\n",
    "    df['wn_topic__LCH'] = lch_list\n",
    "    df['wn_topic__JC'] = jc_list\n",
    "    df['wn_topic__LIN'] = lin_list\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c34f3-05d0-4082-b3cd-18e0d7236eac",
   "metadata": {},
   "source": [
    "Compare WN pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d0f75-c547-4329-bcff-69f40835d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from tqdm import tqdm\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet_ic')\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "# Cache synset lookup\n",
    "@lru_cache(maxsize=10000)\n",
    "def get_first_noun_synset(word):\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    return synsets[0] if synsets else None\n",
    "\n",
    "# Cache similarity computation between synsets\n",
    "@lru_cache(maxsize=100000)\n",
    "def get_similarities_cached(word1, word2):\n",
    "    s1 = get_first_noun_synset(word1)\n",
    "    s2 = get_first_noun_synset(word2)\n",
    "    if not s1 or not s2:\n",
    "        return 0, 0, 0, 0, 0\n",
    "    try:\n",
    "        wp = s1.wup_similarity(s2) or 0\n",
    "    except:\n",
    "        wp = 0\n",
    "    try:\n",
    "        lch = s1.lch_similarity(s2) or 0\n",
    "    except:\n",
    "        lch = 0\n",
    "    try:\n",
    "        jc = s1.jcn_similarity(s2, brown_ic) or 0\n",
    "    except:\n",
    "        jc = 0\n",
    "    try:\n",
    "        lin = s1.lin_similarity(s2, brown_ic) or 0\n",
    "    except:\n",
    "        lin = 0\n",
    "    try:\n",
    "        path = s1.path_similarity(s2) or 0\n",
    "    except:\n",
    "        path = 0\n",
    "    return wp, lch, jc, lin, path\n",
    "\n",
    "# Main function\n",
    "def compute_pairwise_similarities_fast(df):\n",
    "    tqdm.pandas()\n",
    "\n",
    "    def process(tokens):\n",
    "        n = len(tokens)\n",
    "        wp = np.zeros(n)\n",
    "        lch = np.zeros(n)\n",
    "        jc = np.zeros(n)\n",
    "        lin = np.zeros(n)\n",
    "        path = np.zeros(n)\n",
    "        counts = np.zeros(n)\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                w1, w2 = tokens[i], tokens[j]\n",
    "                sim_wp, sim_lch, sim_jc, sim_lin, sim_path = get_similarities_cached(w1, w2)\n",
    "\n",
    "                wp[i] += sim_wp\n",
    "                wp[j] += sim_wp\n",
    "                lch[i] += sim_lch\n",
    "                lch[j] += sim_lch\n",
    "                jc[i] += sim_jc\n",
    "                jc[j] += sim_jc\n",
    "                lin[i] += sim_lin\n",
    "                lin[j] += sim_lin\n",
    "                path[i] += sim_path\n",
    "                path[j] += sim_path\n",
    "                counts[i] += 1\n",
    "                counts[j] += 1\n",
    "\n",
    "        # Avoiding error by /0 division\n",
    "        wp = np.divide(wp, counts, out=np.zeros_like(wp), where=counts != 0)\n",
    "        lch = np.divide(lch, counts, out=np.zeros_like(lch), where=counts != 0)\n",
    "        jc = np.divide(jc, counts, out=np.zeros_like(jc), where=counts != 0)\n",
    "        lin = np.divide(lin, counts, out=np.zeros_like(lin), where=counts != 0)\n",
    "        path = np.divide(path, counts, out=np.zeros_like(path), where=counts != 0)\n",
    "\n",
    "        return pd.Series([wp.tolist(), lch.tolist(), jc.tolist(), lin.tolist(), path.tolist()])\n",
    "\n",
    "    df[['wn_pair__WP', 'wn_pair__LCH', 'wn_pair__JC', 'wn_pair__LIN', 'wn_pair__PATH']] = df['tokenized_sentences'].progress_apply(process)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203426ad-105d-472f-8091-894eddaa8e2c",
   "metadata": {},
   "source": [
    "Average similarity of glove (SEQUENTIALLY embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf6ed50-7fba-4d32-87d4-60b0e6cd2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert the stringified lists to actual lists\n",
    "df['tokenized_sentences'] = df['tokenized_sentences'].apply(ast.literal_eval)\n",
    "\n",
    "# Now apply the logic correctly\n",
    "df['avg_sim_glove_repeated'] = df.apply(\n",
    "    lambda row: [row['avg_sim_glove']] * len(row['tokenized_sentences']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c803fb6-7a40-45e8-bb00-738dd386dc2f",
   "metadata": {},
   "source": [
    "POS tagging sequentially (not used in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e663d6f-b7eb-493c-b225-73309329e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pos_tags_list = []\n",
    "\n",
    "for sentence in tqdm(df['sentence']):\n",
    "    doc = nlp(sentence)\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    pos_tags_list.append(pos_tags)\n",
    "\n",
    "df['pos_tags'] = pos_tags_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c321af0-58e7-4447-98b4-e5aa21e479a0",
   "metadata": {},
   "source": [
    "Preprocessing target column (into 1 (metaphorical) and 0 (literal/non-metaphorical) words) The method of doing this spans multiple cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ad34e-eb2f-48cf-a4fd-be2428b04390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conceptual metaphor column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd630ab-ee7c-48d2-984f-6dbc72d083a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = pd.read_excel(\"/Users/j/Documents/thesis/dyn.xlsx\", sheet_name=\"trump\")\n",
    "clinton = pd.read_excel(\"/Users/j/Documents/thesis/dyn.xlsx\", sheet_name=\"clinton\")\n",
    "df = pd.concat([trump, clinton], ignore_index=True)\n",
    "\n",
    "df = df.loc[:, ~df.columns.str.startswith('SD_')]\n",
    "df = df.iloc[:, 19:].reset_index(drop=True)\n",
    "df['sentence'] = df['collocate left'].astype(str).fillna('') + ' ' + df['Kwic'].astype(str).fillna('') + ' ' + df['collocate right'].astype(str).fillna('')\n",
    "\n",
    "df2 = df[['sentence', 'conceptual metaphor']]\n",
    "df2['sentence'] = df2['sentence'].str.lower().fillna('')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c5f5f-e6fe-4480-9902-afc4b1ff03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only between parenthesis (first word)\n",
    "\n",
    "def extract_last_parentheses(text):\n",
    "    if isinstance(text, str):\n",
    "        matches = re.findall(r'\\(([^()]*)\\)', text)\n",
    "        return matches[-1].strip().lower() if matches else None\n",
    "    else:\n",
    "        return text \n",
    "\n",
    "df['conceptual_metaphor'] = df['conceptual_metaphor'].apply(extract_last_parentheses)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab24e5f7-34f5-4a90-a8ea-2beeafec6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if none, replace with word in basic_word_first .. . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad3f41-7a36-45f0-bd29-6d011ff03d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['conceptual_metaphor'] = df['conceptual_metaphor'].fillna(df['basic_form_first_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36bda6-2c56-49d3-abdf-a9ea255647b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the original column\n",
    "col_index = df.columns.get_loc('conceptual_metaphor') + 1\n",
    "\n",
    "# Insert the duplicate column\n",
    "df.insert(col_index, 'conceptual_metaphor_copy', df['conceptual_metaphor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8545967-8902-4c71-acb4-c71e060aa1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#latere matching\n",
    "import numpy as np\n",
    "\n",
    "# mask: rows where conceptual_metaphor_copy is NaN & matched_token is not NaN\n",
    "mask = df['conceptual_metaphor_copy'].isna() & df['matched_token'].notna()\n",
    "\n",
    "# Fill conceptual_metaphor_copy with the first word of matched_token\n",
    "df.loc[mask, 'conceptual_metaphor_copy'] = df.loc[mask, 'matched_token'].apply(\n",
    "    lambda x: str(x).strip().split()[0] if str(x).strip() else np.nan\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781dd394-c113-4e42-aa81-cdc0b20697bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some abstract code to extract metaphorical word from col.\n",
    "\n",
    "col_name = ' basic form  of  potential metaphorical word with word class'\n",
    "\n",
    "def clean_and_split(text):\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return None\n",
    "    no_parentheses = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    word_list = no_parentheses.strip().split()\n",
    "    return word_list\n",
    "\n",
    "# Apply the function\n",
    "df['word_list'] = df[col_name].apply(clean_and_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298467e-bccd-4507-9eb3-e139d4167dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['conceptual_metaphor_copy'] = df['conceptual_metaphor_copy'].apply(\n",
    "    lambda x: x.split() if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c8e09-30e3-4817-8c23-d9a7ac8fd5b4",
   "metadata": {},
   "source": [
    "make sure the word form is the same as in the sentence. So if the stem is noted, find the right form in the actual sentence and adjust accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb94033-e330-4ecb-ac25-af0e115d3164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "#get word(s) from the 'metaphor' col\n",
    "def parse_metaphor(metaphor_str):\n",
    "    metaphor_str = str(metaphor_str)\n",
    "    match = re.search(r'\\[(.*?)\\]', metaphor_str)\n",
    "    if match:\n",
    "        content = match.group(1)\n",
    "        words = [re.sub(r'[^a-zA-Z]', '', word.strip()) for word in content.split(',')]\n",
    "        return [w for w in words if w]\n",
    "    return []\n",
    "\n",
    "#find most similar words in the tokenized sentence.\n",
    "def find_sequential_match(sentence_words, target_words):\n",
    "    if not target_words or not sentence_words:\n",
    "        return []\n",
    "    \n",
    "    matched_words = []\n",
    "    search_start = 0\n",
    "    \n",
    "    for target in target_words:\n",
    "        best_score = -1\n",
    "        best_word = None\n",
    "        best_index = -1\n",
    "        \n",
    "        for i in range(search_start, len(sentence_words)):\n",
    "            score = fuzz.ratio(target, sentence_words[i])\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_word = sentence_words[i]\n",
    "                best_index = i\n",
    "        \n",
    "        if best_word is not None:\n",
    "            matched_words.append(best_word)\n",
    "            search_start = best_index + 1\n",
    "        else:\n",
    "            matched_words.append(None)\n",
    "    \n",
    "    return matched_words\n",
    "\n",
    "df['matched_words'] = df.progress_apply(\n",
    "    lambda row: find_sequential_match(row['tokenized_sentences'], parse_metaphor(row['conceptual_metaphor_copy'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7968e-a246-4ae9-b742-2efe23638b3c",
   "metadata": {},
   "source": [
    "The final code to compare matched_words with the words in tokenized_sentence; it makes a new sequential column, where metaphorical words are annotated with a 1 and nonmetaphorical words are annotated with a 0.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afdee76-c159-44ac-a659-e9ae01e610dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row_for_pattern(row):\n",
    "    sentence_raw = row['tokenized_sentences']\n",
    "    pattern_raw = row['matched_words']\n",
    "\n",
    "    #As between sessions i loaded from a saved excell sheet sometimes the types were ruined dwhich therefore had to be fixed\n",
    "    try:\n",
    "        sentence = ast.literal_eval(sentence_raw) if isinstance(sentence_raw, str) else sentence_raw\n",
    "        pattern = ast.literal_eval(pattern_raw) if isinstance(pattern_raw, str) else pattern_raw\n",
    "\n",
    "    if not isinstance(sentence, list) or not isinstance(pattern, list):\n",
    "        print(f\"Invalid input types in row {row.name}: sentence={type(sentence)}, pattern={type(pattern)}\")\n",
    "        return [0] * len(sentence) if isinstance(sentence, list) else []\n",
    "\n",
    "    #actual code\n",
    "    if not sentence or not pattern:\n",
    "        return [0] * len(sentence) if sentence else []\n",
    "\n",
    "    result = [0] * len(sentence)\n",
    "    pattern_idx = 0  # Tracks the next word to match in pattern\n",
    "\n",
    "    for i, token in enumerate(sentence):\n",
    "        if pattern_idx < len(pattern) and token == pattern[pattern_idx]:\n",
    "            result[i] = 1\n",
    "            pattern_idx += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "df['match_result'] = df.apply(process_row_for_pattern, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d13fa9-cd84-46d6-8438-681199a99d06",
   "metadata": {},
   "source": [
    "Below the code for the training of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b98d5-9645-4b53-a2d6-c22234776e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Configuration and determinism \n",
    "MODEL_NAME = 'bert-large-uncased'\n",
    "BATCH_SIZE = 8\n",
    "MAX_LEN = 128\n",
    "NUM_EPOCHS = 20\n",
    "PATIENCE = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class MetaphorDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=MAX_LEN):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        tokens = row['tokenized_sentences']\n",
    "        labels = row['match_result']\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "\n",
    "        word_ids = encoding.word_ids()\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        aligned_labels = []\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                try:\n",
    "                    aligned_labels.append(labels[word_id])\n",
    "                except IndexError:\n",
    "                    aligned_labels.append(-100)\n",
    "\n",
    "        labels_tensor = torch.tensor(aligned_labels, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels_tensor\n",
    "        }\n",
    "\n",
    "# Model\n",
    "class MetaphorClassifier(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super(MetaphorClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        hidden_dim = self.bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        seq_output = self.dropout(outputs.last_hidden_state)\n",
    "        logits = self.classifier(seq_output)\n",
    "        return logits\n",
    "\n",
    "# splitting\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = MetaphorDataset(train_df, tokenizer)\n",
    "val_dataset = MetaphorDataset(val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Initialize model on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MetaphorClassifier().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(logits.view(-1, 2), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(logits.view(-1, 2), labels.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Evaluation\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(logits, dim=-1).view(-1).cpu().numpy()\n",
    "        labels = labels.view(-1).cpu().numpy()\n",
    "\n",
    "        mask = labels != -100\n",
    "        all_preds.extend(preds[mask])\n",
    "        all_labels.extend(labels[mask])\n",
    "\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf74ec-6be5-4ccd-8add-7f437a1eaee4",
   "metadata": {},
   "source": [
    "the same exact code was ran with the other models, replace MODEL_NAME with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc0050-77e1-4108-894d-ebfd46be1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the transformer library.\n",
    "MODEL_NAME = 'bert-large-uncased'\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
    "MODEL_NAME = 'microsoft/deberta-v3-large'\n",
    "MODEL_NAME = 'roberta-large'\n",
    "MODEL_NAME = 'roberta-base'\n",
    "\n",
    "\n",
    "MODEL_NAME = 'roberta-base' #used in combination with the roberta tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ff650-af9b-4900-b1d2-e2db8cbc3217",
   "metadata": {},
   "source": [
    "Code for ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eca8f7-6810-44ad-8bb8-893cd3640cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Feature columns\n",
    "FEATURE_COLUMNS = [\n",
    "    'wn_pair__WP', 'wn_pair__LCH', 'wn_pair__JC', 'wn_pair__LIN', 'wn_pair__PATH',\n",
    "    'wn_topic__WUP', 'wn_topic__LCH', 'wn_topic__JC', 'wn_topic__LIN', 'wn_topic__PATH', 'glove'\n",
    "]\n",
    "\n",
    "# determinism\n",
    "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 20\n",
    "PATIENCE = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class MetaphorDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, feature_name, max_len=MAX_LEN):\n",
    "        self.feature_name = feature_name\n",
    "        self.labels = df['match_result'].tolist()\n",
    "        self.features = df[feature_name].tolist()\n",
    "\n",
    "        self.inputs = tokenizer(\n",
    "            df['tokenized_sentences'].tolist(),\n",
    "            is_split_into_words=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_len\n",
    "        )\n",
    "\n",
    "        # Save word_ids separately\n",
    "        self.word_ids = [\n",
    "            self.inputs.word_ids(batch_index=i)\n",
    "            for i in range(len(self.labels))\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs['input_ids'][idx]\n",
    "        attention_mask = self.inputs['attention_mask'][idx]\n",
    "        word_ids = self.word_ids[idx]\n",
    "\n",
    "        feature_vecs = []\n",
    "        aligned_labels = []\n",
    "\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                feature_vecs.append([0.0])\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                try:\n",
    "                    val = self.features[idx][word_id]\n",
    "                    feature_vecs.append([float(val)])\n",
    "                    aligned_labels.append(self.labels[idx][word_id])\n",
    "                except Exception:\n",
    "                    feature_vecs.append([0.0])\n",
    "                    aligned_labels.append(-100)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'features': torch.tensor(feature_vecs, dtype=torch.float32),\n",
    "            'labels': torch.tensor(aligned_labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Model\n",
    "class MetaphorClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim=1, model_name=MODEL_NAME):\n",
    "        super(MetaphorClassifier, self).__init__()\n",
    "        self.deberta = AutoModel.from_pretrained(model_name)\n",
    "        hidden_dim = self.deberta.config.hidden_size\n",
    "\n",
    "        self.feature_proj = nn.Linear(feature_dim, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim + 64, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, features):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        seq_output = outputs.last_hidden_state\n",
    "\n",
    "        projected_feats = self.feature_proj(features)\n",
    "        combined = torch.cat([seq_output, projected_feats], dim=-1)\n",
    "        combined = self.dropout(combined)\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n",
    "\n",
    "# Run ablation study\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for feature in FEATURE_COLUMNS:\n",
    "    print(f\"\\{feature}\")\n",
    "    set_seed(42)\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_dataset = MetaphorDataset(train_df, tokenizer, feature)\n",
    "    val_dataset = MetaphorDataset(val_df, tokenizer, feature)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "    model = MetaphorClassifier().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training [{feature}] Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            features = batch['features'].to(device, non_blocking=True)\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(input_ids, attention_mask, features)\n",
    "                logits = logits.view(-1, 2)\n",
    "                labels = labels.view(-1)\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validation [{feature}]\"):\n",
    "                input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "                features = batch['features'].to(device, non_blocking=True)\n",
    "                labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = model(input_ids, attention_mask, features)\n",
    "                    logits = logits.view(-1, 2)\n",
    "                    labels = labels.view(-1)\n",
    "                    loss = loss_fn(logits, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f} | Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f'best_model_{feature}.pt')\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= PATIENCE:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Evaluation\n",
    "    model.load_state_dict(torch.load(f'best_model_{feature}.pt'))\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Evaluating [{feature}]\"):\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            features = batch['features'].to(device, non_blocking=True)\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(input_ids, attention_mask, features)\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1).view(-1).cpu().numpy()\n",
    "            labels = labels.view(-1).cpu().numpy()\n",
    "            mask = labels != -100\n",
    "            all_preds.extend(preds[mask])\n",
    "            all_labels.extend(labels[mask])\n",
    "\n",
    "    print(f\"\\nClassification Report for feature: {feature}\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
